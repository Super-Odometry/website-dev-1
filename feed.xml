<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://theairlab.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://theairlab.org/" rel="alternate" type="text/html" /><updated>2022-06-10T20:06:28+00:00</updated><id>https://theairlab.org/feed.xml</id><title type="html">AirLab</title><subtitle>Researching, developing, and testing autonomous flying robots at Carnegie Mellon University
</subtitle><entry><title type="html">Design, Modeling and Control for a Tilt-rotor VTOL UAV in the Presence of Actuator Failure</title><link href="https://theairlab.org/research/2022/05/20/vtol/" rel="alternate" type="text/html" title="Design, Modeling and Control for a Tilt-rotor VTOL UAV in the Presence of Actuator Failure" /><published>2022-05-20T10:40:07+00:00</published><updated>2022-05-20T10:40:07+00:00</updated><id>https://theairlab.org/research/2022/05/20/vtol</id><content type="html" xml:base="https://theairlab.org/research/2022/05/20/vtol/">&lt;p&gt;Enabling vertical take-off and landing while providing the ability to fly long ranges opens the door to a wide range of new real-world aircraft applications while improving many existing tasks. Tiltrotor vertical take-off and landing (VTOL) unmanned aerial vehicles (UAVs) are a better choice than fixed-wing and multirotor aircraft for such applications. Prior works on these aircraft have addressed the aerodynamic performance, design, modeling, and control. However, a less explored area is the study of their potential fault tolerance due to their inherent redundancy, which allows them to tolerate some degree of actuation failure. This work introduces tolerance to several types of actuator failures in a tiltrotor VTOL aircraft. We discuss the design and modeling of a custom tiltrotor VTOL UAV, which is a combination of a fixed-wing aircraft and a quadrotor with tilting rotors, where the four propellers can be rotated individually. Then, we analyze the feasible wrench space the vehicle can generate and design the dynamic control allocation so that the system can adapt to actuator failures, benefiting from the configuration redundancy. The proposed approach is lightweight and is implemented as an extension to an already-existing flight control stack. Extensive experiments validate that the system can maintain the controlled flight under different actuator failures. This work is the first study of the tiltrotor VTOL’s fault-tolerance that exploits the configuration redundancy to the best of our knowledge.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-05-20-vtol/VTOL_PS_Label.PNG&quot; alt=&quot;Tiltrotor VTOL&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;The following video is from the paper submitted to IROS 2022 (under review) that shows the general idea of the new controller design.&lt;/p&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/hrlpgeTy-0g&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;Main contributions of this work includes:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Proposing a dynamic control allocation method that allows the system to adapt to actuator failures. The proposed approach is light-weight and can be quickly extended on an already-existing flight control stack;&lt;/li&gt;
  &lt;li&gt;Designing and modeling a tiltrotor VTOL with the ability to rotate each individual propeller;&lt;/li&gt;
  &lt;li&gt;Validating the system performance under the set of possible actuator failures in different flight phases;&lt;/li&gt;
  &lt;li&gt;Providing the source code for the proposed strategies implemented on the PX4 flight controller firmware along with our simulation environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more detailed information about this work, please refer to the publication&lt;/p&gt;

&lt;h3 id=&quot;publications&quot;&gt;Publications&lt;/h3&gt;

&lt;p&gt;The general ideas on design and modeling of our custom tiltrotor VTOL and desing of the optimization based dynamic control allocation (so that the system can adapt to actuator failures) are described in the following publication (access on &lt;a href=&quot;https://arxiv.org/abs/2205.05533&quot;&gt;arXiv&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;em&gt;BibTeX:&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article{mousaei2022,
author={Mohammadreza Mousaei and Junyi Geng and Azarakhsh Keipour and Dongwei Bai and Sebastian Scherer},
booktitle={arXiv},
title={Design, Modeling and Control for a Tilt-rotor VTOL UAV in the Presence of Actuator Failure}, 
year={in press},
link={https://arxiv.org/abs/2205.05533},
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;IEEE Style:&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;M. Mousaei, J. Geng, A. Keipour, D. Bai, and S. Scherer, “Design, Modeling and Control for a Tilt-rotor VTOL UAV in the Presence of Actuator Failure,”, Under review. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;

&lt;p&gt;Mohammadreza Mousaei (mmousaei [at] cs [dot] cmu [dot] edu)&lt;/p&gt;

&lt;p&gt;Junyi Geng - (junyigen [at] andrew [dot] cmu [dot] edu)&lt;/p&gt;

&lt;p&gt;Azarakhsh Keipour - (keipour [at] cmu [dot] edu)&lt;/p&gt;

&lt;p&gt;Dongwei Bai - (saeedb [at] andrew [dot] cmu [dot] edu)&lt;/p&gt;

&lt;p&gt;Sebastian Scherer - (basti [at] cmu [dot] edu)&lt;/p&gt;</content><author><name>AirLab</name></author><category term="research" /><summary type="html">Enabling vertical take-off and landing while providing the ability to fly long ranges opens the door to a wide range of new real-world aircraft applications while improving many existing tasks. Tiltrotor vertical take-off and landing (VTOL) unmanned aerial vehicles (UAVs) are a better choice than fixed-wing and multirotor aircraft for such applications. Prior works on these aircraft have addressed the aerodynamic performance, design, modeling, and control. However, a less explored area is the study of their potential fault tolerance due to their inherent redundancy, which allows them to tolerate some degree of actuation failure. This work introduces tolerance to several types of actuator failures in a tiltrotor VTOL aircraft. We discuss the design and modeling of a custom tiltrotor VTOL UAV, which is a combination of a fixed-wing aircraft and a quadrotor with tilting rotors, where the four propellers can be rotated individually. Then, we analyze the feasible wrench space the vehicle can generate and design the dynamic control allocation so that the system can adapt to actuator failures, benefiting from the configuration redundancy. The proposed approach is lightweight and is implemented as an extension to an already-existing flight control stack. Extensive experiments validate that the system can maintain the controlled flight under different actuator failures. This work is the first study of the tiltrotor VTOL’s fault-tolerance that exploits the configuration redundancy to the best of our knowledge.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-05-20-vtol/vtol_CAD.gif" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-05-20-vtol/vtol_CAD.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SubT UAV Code Release</title><link href="https://theairlab.org/research/2022/05/02/subt_code/" rel="alternate" type="text/html" title="SubT UAV Code Release" /><published>2022-05-02T10:50:07+00:00</published><updated>2022-05-02T10:50:07+00:00</updated><id>https://theairlab.org/research/2022/05/02/subt_code</id><content type="html" xml:base="https://theairlab.org/research/2022/05/02/subt_code/">&lt;p&gt;This is the code accompanying the paper Resilient Multi-Sensor Exploration of Multifarious Environments with a Team of Aerial Robots [1]. This paper describes team Explorer’s exploration strategy for a team of UAVs in the DARPA SubT competition. The code should be run on an Ubuntu 18.04 system with ROS melodic and OpenCL installed. The procedure for installing OpenCL depends on which type of GPU your system has. If you have an NVIDIA GPU and have CUDA installed, then you should already have OpenCL. You can check by doing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt install clinfo&lt;/code&gt; and running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clinfo&lt;/code&gt;. If it outputs a number of platforms found higher than 0, then OpenCL is installed.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-05-02-subt_code/picture.png&quot; alt=&quot;uav&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;building-the-code-natively&quot;&gt;Building the Code Natively&lt;/h3&gt;

&lt;p&gt;Download and build the code by running the following:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;git clone git@bitbucket.org:castacks/subt_uav.git &lt;br /&gt;
cd subt_uav/ &lt;br /&gt;
./install_dependencies.sh &lt;br /&gt;
./build.sh&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;running-examples-natively&quot;&gt;Running Examples Natively&lt;/h3&gt;

&lt;p&gt;The following commands can be used to launch the UAV in different environments. The first time you launch everything, the gui below will not have the buttons highlighted in blue. To load the buttons, press the Open Config button highlighted in red and select gui.yaml from the open file dialog box. To avoid doing this each time you laucnh the sim, save the perspective using the the Perspectives drop down menu at the top, then selecting Export, and overwriting the core.perspective file.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-05-02-subt_code/gui.png&quot; alt=&quot;GUI&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;First run&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;source devel/setup.bash&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;To launch the UAV in a small room run:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;mon launch core_central sim_main.launch world:=~/subt/final_ws/src/core_gazebo_sim/worlds/room.world&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;To launch the UAV in an indoor hallway environment run:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;mon launch core_central sim_main.launch world:=~/subt/final_ws/src/core_gazebo_sim/worlds/hawkins_qualification.world&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;To launch the UAV in an indoor two story buliding run:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;mon launch core_central sim_main.launch world:=~/subt/final_ws/src/core_gazebo_sim/worlds/filmmakers2.world&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;In addition to the gui shown above, rviz and gazebo windows will also launch and should look like the following:&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-05-02-subt_code/sim.png&quot; alt=&quot;rviz_gazebo&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;building-the-code-in-docker&quot;&gt;Building the Code in Docker&lt;/h3&gt;

&lt;p&gt;Run:&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;cd subt_uav &lt;br /&gt;
./docker_build.sh&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;running-examples-in-docker&quot;&gt;Running Examples in Docker&lt;/h3&gt;

&lt;p&gt;Run:&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;cd subt_uav
./docker_run.sh&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Inside docker, enter the workspace and source it:&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;cd /home/ws &lt;br /&gt;
source devel/setup.bash&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Run a roscore. This can be done from outside docker.&lt;/p&gt;

&lt;p&gt;When you launch the following examples, add buttons to the gui the same way as described at the beginning of the Running Examples Natively section.&lt;/p&gt;

&lt;p&gt;To launch the UAV in a small room run:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;mon launch core_central sim_main.launch world:=/home/ws/src/core_gazebo_sim/worlds/room.world&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;To launch the UAV in an indoor hallway environment run:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;mon launch core_central sim_main.launch world:=/home/ws/src/core_gazebo_sim/worlds/hawkins_qualification.world&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;To launch the UAV in an indoor two story buliding run:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;mon launch core_central sim_main.launch world:=/home/ws/src/core_gazebo_sim/worlds/filmmakers2.world&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;[1] G. Best, R. Garg, J. Keller, G. A. Hollinger, S. Scherer. “Resilient Multi-Sensor Exploration of Multifarious Environments with a Team of Aerial Robots”. Proc. Robotics: Science and Systems, 2022&lt;/p&gt;</content><author><name>AirLab</name></author><category term="research" /><summary type="html">This is the code accompanying the paper Resilient Multi-Sensor Exploration of Multifarious Environments with a Team of Aerial Robots [1]. This paper describes team Explorer’s exploration strategy for a team of UAVs in the DARPA SubT competition. The code should be run on an Ubuntu 18.04 system with ROS melodic and OpenCL installed. The procedure for installing OpenCL depends on which type of GPU your system has. If you have an NVIDIA GPU and have CUDA installed, then you should already have OpenCL. You can check by doing sudo apt install clinfo and running clinfo. If it outputs a number of platforms found higher than 0, then OpenCL is installed.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-05-02-subt_code/collage3c.png" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-05-02-subt_code/collage3c.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">TIGRIS: An Informed Sampling-based Algorithm for Informative Path Planning</title><link href="https://theairlab.org/tigris/" rel="alternate" type="text/html" title="TIGRIS: An Informed Sampling-based Algorithm for Informative Path Planning" /><published>2022-03-25T02:00:07+00:00</published><updated>2022-03-25T02:00:07+00:00</updated><id>https://theairlab.org/tigris</id><content type="html" xml:base="https://theairlab.org/tigris/">&lt;p&gt;Informative path planning is an important and challenging problem in robotics that remains to be solved in a manner that allows for wide-spread implementation and real-world practical adoption. Among various reasons for this, one is the lack of approaches that allow for informative path planning in high-dimensional spaces and non-trivial sensor constraints. In this work we present a sampling-based approach that allows us to tackle the challenges of large and high-dimensional search spaces. This is done by performing informed sampling in the high-dimensional continuous space and incorporating potential information gain along edges in the reward estimation. This method rapidly generates a global path that maximizes information gain for the given path budget constraints.&lt;/p&gt;

&lt;!-- &lt;figure&gt;
 &lt;img src=&quot;/img/posts/2021-08-31-xplane-ros/front_view.png&quot; style=&quot;width:49%&quot; /&gt;
  &lt;img src=&quot;/img/posts/2021-08-31-xplane-ros/side_view.png&quot; style=&quot;width:49%&quot; /&gt;
 &lt;figcaption&gt;
XPlaneROS integrates XPlane 11 with ROSplane to enable higher-level autonomy in general aviation.
 &lt;/figcaption&gt;
&lt;/figure&gt; --&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-03-25-tigris/fig-finalv4.jpg&quot; style=&quot;width:59%&quot; /&gt;
 &lt;figcaption&gt;
An example scenario for an informative path planning problem of searching for a missing hiker. The UAV plans a path of maximum information gain over a prior belief distribution of the hiker's location.
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;testing-implementation&quot;&gt;Testing Implementation&lt;/h2&gt;
&lt;p&gt;We implemented TIGRIS with the objective of reducing entropy with a bias toward increasing belief probabilities. Our data gathering is performed using a fixed wing UAV and a static forward facing camera.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-03-25-tigris/SensorModelv4.png&quot; style=&quot;width:59%&quot; /&gt;
 &lt;figcaption&gt;
A visualization of the edge reward approximation and variable definitions given a fixed forward-facing camera.
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;
&lt;p&gt;We compare our approach to a sampling-based planner baseline and run 12,000 Monte Carlo simulations with 1-12 randomly placed target belief centroids. Through testing we show how our contributions allow our approach to consistently out-perform the baseline by 18.0%. An example test result is shown in the figure below. Detailed results and analysis can be found in our &lt;a href=&quot;https://arxiv.org/abs/2203.12830&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-03-25-tigris/ExampleRunv8.png&quot; style=&quot;width:69%&quot; /&gt;
 &lt;figcaption&gt;
Example results in the testing environment. Red regions are areas of high reward and blue is low reward. The blue lines is the TIGRIS solution and the pink line is baseline planner. The bottom right image shows a 3D view of the environment, and the bottom left graph shows the best path reward of the planners over time.
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;
&lt;p&gt;The video below gives a general overview of the planner and visualization of a potential application.&lt;/p&gt;
&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/bMw5nUGL5GQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/abs/2203.12830&quot;&gt;paper&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Future work directions include replanning on the fly with TIGRIS, searching for and tracking moving target, local optimization of the final path, and local optimization over path segments within the planning framework.&lt;/p&gt;

&lt;h2 id=&quot;additional-info&quot;&gt;Additional Info&lt;/h2&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{mo2022_tigris,
  doi = {10.48550/ARXIV.2203.12830},
  url = {https://arxiv.org/abs/2203.12830},
  author = {Moon, Brady and Chatterjee, Satrajit and Scherer, Sebastian},
  title = {TIGRIS: An Informed Sampling-based Algorithm for Informative Path Planning},
  publisher = {arXiv},
  year = {2022},
  video = {https://youtu.be/bMw5nUGL5GQ}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;contributors&quot;&gt;Contributors&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://theairlab.org/team/bradym/&quot;&gt;Brady Moon&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://theairlab.org/team/satrajit_chatterjee/&quot;&gt;Satrajit Chatterjee&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://theairlab.org/team/sebastian/&quot;&gt;Dr. Sebastian Scherer&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;term-of-use&quot;&gt;Term of use&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://choosealicense.com/licenses/bsd-4-clause/&quot;&gt;BSD 4-Clause License&lt;/a&gt;&lt;/p&gt;

&lt;!-- &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border-width:0&quot; src=&quot;https://i.creativecommons.org/l/by/4.0/80x15.png&quot; /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;. --&gt;</content><author><name>Brady Moon</name></author><category term="research" /><summary type="html">Informative path planning is an important and challenging problem in robotics that remains to be solved in a manner that allows for wide-spread implementation and real-world practical adoption. Among various reasons for this, one is the lack of approaches that allow for informative path planning in high-dimensional spaces and non-trivial sensor constraints. In this work we present a sampling-based approach that allows us to tackle the challenges of large and high-dimensional search spaces. This is done by performing informed sampling in the high-dimensional continuous space and incorporating potential information gain along edges in the reward estimation. This method rapidly generates a global path that maximizes information gain for the given path budget constraints.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-03-25-tigris/IROSCover8.png" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-03-25-tigris/IROSCover8.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AirObject: A Temporally Evolving Graph Embedding for Object Identification</title><link href="https://theairlab.org/airobject/" rel="alternate" type="text/html" title="AirObject: A Temporally Evolving Graph Embedding for Object Identification" /><published>2022-03-15T12:00:00+00:00</published><updated>2022-03-15T12:00:00+00:00</updated><id>https://theairlab.org/airobject</id><content type="html" xml:base="https://theairlab.org/airobject/">&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_1_org.gif&quot; /&gt;
    &lt;figcaption&gt;
       Video Object Identification
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Object encoding and identification are vital for robotic tasks such as autonomous exploration, semantic scene understanding, and re-localization. Previous approaches have attempted to either track objects or generate descriptors for object identification. However, such systems are limited to a “fixed” object representation from a single viewpoint and are not robust to severe occlusion, viewpoint shift, perceptual aliasing, or scale transform. These single frame representations tend to lead to false correspondences amongst perceptually-aliased objects, especially when severely occluded. Hence, we propose one of the first temporal object encoding methods, &lt;strong&gt;AirObject&lt;/strong&gt;, that aggregates the temporally “evolving” object structure as the camera or object moves. The AirObject descriptors, which accumulate knowledge across multiple evolving representations of the objects, are robust to severe occlusion, viewpoint changes, deformation, perceptual aliasing, and the scale transform.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/overview.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Matching Temporally Evolving Representations using AirObject
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;topological-object-representations&quot;&gt;Topological Object Representations&lt;/h2&gt;

&lt;p&gt;Intuitively, a group of feature points on an object form a graphical representation where the feature points are nodes and their associated descriptors are the node features. Essentially, the graph’s nodes are the distinctive local features of the object, while the edges/structure of the graph represents the global structure of the object. Hence, to learn the geometric relationship of the feature points, we construct topological object graphs for each frame using Delaunay triangulation. The obtained triangular mesh representation enables the graph attention encoder to reason better about the object’s structure, thereby making the final temporal object descriptor robust to deformation or occlusion.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/triangulation.png&quot; /&gt;
    &lt;figcaption&gt;
        Topological Graph Representations of Objects. These representations are generated by using Delaunay Triangulation on object-wise grouped SuperPoint keypoints.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;simple--effective-temporal-object-encoding-method&quot;&gt;Simple &amp;amp; Effective Temporal Object Encoding Method&lt;/h2&gt;

&lt;p&gt;Our proposed method is very simple and only contains three modules. Specifically, we use extracted deep learned keypoint features across multiple frames to form sequences of object-wise topological graph neural networks (GNNs), which on embedding generate temporal object descriptors. We employ a graph attention-based sparse encoding method on these topological GNNs to generate content graph features and location graph features representing the structural information of the object. Then, these graph features are aggregated across multiple frames using a single-layer temporal convolutional network to generate a temporal object descriptor. These generated object descriptors are robust to severe occlusion, perceptual aliasing, viewpoint shift, deformation, and scale transform, outperforming the state-of-the-art single-frame and sequential descriptors.&lt;/p&gt;

&lt;figure&gt;
    &lt;figcaption&gt;
       Video Object Identification
    &lt;/figcaption&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_2.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_3.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_4.gif&quot; /&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/lTEXcKW_aWg&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{keetha2022airobject,
  title     = {AirObject: A Temporally Evolving Graph Embedding for Object Identification},
  author    = {Keetha, Nikhil Varma and Wang, Chen and Qiu, Yuheng and Xu, Kuan and Scherer, Sebastian}, 
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2111.15150}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/abs/2111.15150&quot;&gt;Paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/Nik-V9/AirObject&quot;&gt;Code&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nik-v9.github.io/&quot;&gt;Nikhil Varma Keetha&lt;/a&gt; &amp;lt;keethanikhil [at] gmail [dot] com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Nikhil Varma Keetha</name></author><category term="research" /><summary type="html">Video Object Identification</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-03-15-airobject/obj_1_org.gif" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-03-15-airobject/obj_1_org.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Air Series Articles Prereleased</title><link href="https://theairlab.org/airseries/" rel="alternate" type="text/html" title="Air Series Articles Prereleased" /><published>2022-03-12T00:00:00+00:00</published><updated>2022-03-12T00:00:00+00:00</updated><id>https://theairlab.org/air-series</id><content type="html" xml:base="https://theairlab.org/airseries/">&lt;p&gt;&lt;strong&gt;Air Series&lt;/strong&gt; is a collection of &lt;strong&gt;articles mentored by &lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A wide variety of topics in &lt;strong&gt;robotics&lt;/strong&gt; are covered, including &lt;strong&gt;localization&lt;/strong&gt;, &lt;strong&gt;detection&lt;/strong&gt;, and &lt;strong&gt;lifelong learning&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;All articles are &lt;strong&gt;first authored by Undergraduate or Master students&lt;/strong&gt; and &lt;strong&gt;second authored by Chen Wang&lt;/strong&gt;.&lt;/p&gt;

&lt;style&gt;
.csl-block {
    font-size: 16px;
}
.csl-title, .csl-author, .csl-event, .csl-editor, .csl-venue {
    display: block;
    position: relative;
    font-size: 15px;
}

.csl-title b {
    font-weight: 600;
}

.csl-content {
    display: inline-block;
    vertical-align: top;
    padding-left: 20px;
}

.bibliography {
   list-style-type: none;
}
&lt;/style&gt;

&lt;h2 id=&quot;air-series-articles&quot;&gt;Air Series Articles&lt;/h2&gt;

&lt;ul.no-bullet class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;li2021siamfind&quot;&gt;[1]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;By Li, B., Wang, C. and Scherer, S.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;In &lt;i&gt;arXiv preprint arXiv:2112.01740&lt;/i&gt;, 2021.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;toggleli2021siamfind()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function toggleli2021siamfind() {
        var x= document.getElementById('ali2021siamfind');
        // console.log(&quot;haha %o&quot;,typeof li2021siamfind);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;


&lt;script&gt;
    function toggle2li2021siamfind() {
        var x= document.getElementById('bli2021siamfind');
        // console.log(&quot;haha %o&quot;,typeof li2021siamfind);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;




&lt;a href=&quot;https://arxiv.org/pdf/2112.01740&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;link&quot; /&gt;&lt;/a&gt;


&lt;!--  --&gt;
&lt;/div&gt;

&lt;div id=&quot;ali2021siamfind&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{li2021siamfind,
  title = {AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration},
  author = {Li, Bowen and Wang, Chen and Scherer, Sebastian},
  booktitle = {arXiv preprint arXiv:2112.01740},
  year = {2021},
  url = {https://arxiv.org/pdf/2112.01740},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;div id=&quot;bli2021siamfind&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;keetha2022airobject&quot;&gt;[2]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirObject: A Temporally Evolving Graph Embedding for Object Identification&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;By Keetha, N.V., Wang, C., Qiu, Y., Xu, K. and Scherer, S.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;In &lt;i&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglekeetha2022airobject()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglekeetha2022airobject() {
        var x= document.getElementById('akeetha2022airobject');
        // console.log(&quot;haha %o&quot;,typeof keetha2022airobject);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;


&lt;script&gt;
    function toggle2keetha2022airobject() {
        var x= document.getElementById('bkeetha2022airobject');
        // console.log(&quot;haha %o&quot;,typeof keetha2022airobject);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;




&lt;a href=&quot;https://arxiv.org/pdf/2111.15150&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;link&quot; /&gt;&lt;/a&gt;


&lt;!--  --&gt;
&lt;/div&gt;

&lt;div id=&quot;akeetha2022airobject&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{keetha2022airobject,
  title = {AirObject: A Temporally Evolving Graph Embedding for Object Identification},
  author = {Keetha, Nikhil Varma and Wang, Chen and Qiu, Yuheng and Xu, Kuan and Scherer, Sebastian},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2022},
  url = {https://arxiv.org/pdf/2111.15150},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;div id=&quot;bkeetha2022airobject&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;gao2021lifelong&quot;&gt;[3]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirLoop: Lifelong Loop Closure Detection&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;By Gao, D., Wang, C. and Scherer, S.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;In &lt;i&gt;International Conference on Robotics and Automation (ICRA)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglegao2021lifelong()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglegao2021lifelong() {
        var x= document.getElementById('agao2021lifelong');
        // console.log(&quot;haha %o&quot;,typeof gao2021lifelong);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;


&lt;script&gt;
    function toggle2gao2021lifelong() {
        var x= document.getElementById('bgao2021lifelong');
        // console.log(&quot;haha %o&quot;,typeof gao2021lifelong);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;




&lt;a href=&quot;https://arxiv.org/pdf/2109.08975&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;link&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/Gr9i5ONNmz0&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button5&quot; value=&quot;video&quot; /&gt;&lt;/a&gt;

&lt;!--  --&gt;
&lt;!-- &lt;a href=&quot;https://github.com/wang-chen/AirLoop&quot;&gt;&lt;input type='button' class='button3' value='code'/&gt;&lt;/a&gt; --&gt;
&lt;!--  --&gt;
&lt;/div&gt;

&lt;div id=&quot;agao2021lifelong&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{gao2021lifelong,
  title = {AirLoop: Lifelong Loop Closure Detection},
  author = {Gao, Dasong and Wang, Chen and Scherer, Sebastian},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year = {2022},
  url = {https://arxiv.org/pdf/2109.08975},
  code = {https://github.com/wang-chen/AirLoop},
  video = {https://youtu.be/Gr9i5ONNmz0},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;div id=&quot;bgao2021lifelong&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;qiu2021airdos&quot;&gt;[4]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirDOS: Visual SLAM Benefits from Dynamic Objects&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;By Qiu, Y., Wang, C., Wang, W., Henein, M. and Scherer, S.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;In &lt;i&gt;International Conference on Robotics and Automation (ICRA)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;toggleqiu2021airdos()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function toggleqiu2021airdos() {
        var x= document.getElementById('aqiu2021airdos');
        // console.log(&quot;haha %o&quot;,typeof qiu2021airdos);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;


&lt;script&gt;
    function toggle2qiu2021airdos() {
        var x= document.getElementById('bqiu2021airdos');
        // console.log(&quot;haha %o&quot;,typeof qiu2021airdos);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;




&lt;a href=&quot;https://arxiv.org/pdf/2109.09903&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;link&quot; /&gt;&lt;/a&gt;


&lt;!--  --&gt;
&lt;!-- &lt;a href=&quot;https://github.com/haleqiu/airdos&quot;&gt;&lt;input type='button' class='button3' value='code'/&gt;&lt;/a&gt; --&gt;
&lt;!--  --&gt;
&lt;/div&gt;

&lt;div id=&quot;aqiu2021airdos&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{qiu2021airdos,
  title = {AirDOS: Visual SLAM Benefits from Dynamic Objects},
  author = {Qiu, Yuheng and Wang, Chen and Wang, Wenshan and Henein, Mina and Scherer, Sebastian},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year = {2022},
  url = {https://arxiv.org/pdf/2109.09903},
  code = {https://github.com/haleqiu/airdos},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;div id=&quot;bqiu2021airdos&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;xu2021aircode&quot;&gt;[5]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirCode: A Robust Object Encoding Method&lt;/b&gt;. &lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;By Xu, K., Wang, C., Chen, C., Wu, W. and Sebastian, S.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;In &lt;i&gt;IEEE Robotics and Automation Letters (RA-L)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglexu2021aircode()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglexu2021aircode() {
        var x= document.getElementById('axu2021aircode');
        // console.log(&quot;haha %o&quot;,typeof xu2021aircode);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;


&lt;script&gt;
    function toggle2xu2021aircode() {
        var x= document.getElementById('bxu2021aircode');
        // console.log(&quot;haha %o&quot;,typeof xu2021aircode);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    &lt;/script&gt;




&lt;a href=&quot;https://arxiv.org/pdf/2105.00327&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;link&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/ZhW4Qk1tLNQ&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button5&quot; value=&quot;video&quot; /&gt;&lt;/a&gt;

&lt;!--  --&gt;
&lt;!-- &lt;a href=&quot;https://github.com/wang-chen/AirCode&quot;&gt;&lt;input type='button' class='button3' value='code'/&gt;&lt;/a&gt; --&gt;
&lt;!--  --&gt;
&lt;/div&gt;

&lt;div id=&quot;axu2021aircode&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@article{xu2021aircode,
  title = {AirCode: A Robust Object Encoding Method},
  author = {Xu, Kuan and Wang, Chen and Chen, Chao and Wu, Wei and Sebastian, Scherer},
  journal = {IEEE Robotics and Automation Letters (RA-L)},
  year = {2022},
  url = {https://arxiv.org/pdf/2105.00327},
  code = {https://github.com/wang-chen/AirCode},
  video = {https://youtu.be/ZhW4Qk1tLNQ},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;div id=&quot;bxu2021aircode&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;&lt;/ul.no-bullet&gt;

&lt;h3 id=&quot;first-author-information-when-work-was-done&quot;&gt;First Author Information (When work was done)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jaraxxus-me.github.io/&quot;&gt;Bowen Li&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;RISS intern at Carnegie Mellon University.&lt;/li&gt;
      &lt;li&gt;Junior student at Tongji University, China.&lt;/li&gt;
      &lt;li&gt;Now: Incoming PhD student of &lt;a href=&quot;https://www.ri.cmu.edu/&quot;&gt;CMU RI&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/nikhil-varma-keetha-612685193/&quot;&gt;Nikhil Varma Keetha&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;RISS intern at Carnegie Mellon University.&lt;/li&gt;
      &lt;li&gt;Junior student at Indian Institute of Technology Dhanbad.&lt;/li&gt;
      &lt;li&gt;Now: Incoming Master student of &lt;a href=&quot;https://www.ri.cmu.edu/&quot;&gt;CMU RI&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scholar.google.com/citations?user=_loctXsAAAAJ&amp;amp;hl=en&quot;&gt;Dasong Gao&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Master student at Carnegie Mellon University.&lt;/li&gt;
      &lt;li&gt;Now: Incoming PhD student of &lt;a href=&quot;https://www.eecs.mit.edu/&quot;&gt;MIT EECS&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scholar.google.com/citations?user=aEK45mEAAAAJ&quot;&gt;Yuheng Qiu&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Undergraduate of Chinese University of Hong Kong.&lt;/li&gt;
      &lt;li&gt;Now: PhD student of &lt;a href=&quot;https://www.meche.engineering.cmu.edu/&quot;&gt;CMU ME&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kuan Xu
    &lt;ul&gt;
      &lt;li&gt;Master Graduate of Harbin Institute of Technology, China.&lt;/li&gt;
      &lt;li&gt;Engineer at Tencent and Geekplus.&lt;/li&gt;
      &lt;li&gt;Now: Incoming PhD student of &lt;a href=&quot;https://www.ntu.edu.sg/eee&quot;&gt;NTU EEE&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contribution&quot;&gt;Contribution&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirDet: Few-shot Detection without Fine-tunning&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first practical few-shot object detection method that requires no fine-tunning.&lt;/li&gt;
      &lt;li&gt;It achieves even better results than the exhaustively fine-tuned methods (up to 60% improvements).&lt;/li&gt;
      &lt;li&gt;Validated on real world sequences from DARPA Subterranean (SubT) challenge.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-12-air-series/AirDet.gif&quot; /&gt;
    &lt;figcaption&gt;
        Only three examples are given for novel object detection without fine-tunning.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirObject: Temporal Object Embedding&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first temporal object embedding method.&lt;/li&gt;
      &lt;li&gt;It achieves the state-of-the-art performance for video object identification.&lt;/li&gt;
      &lt;li&gt;Robust to severe occlusion, perceptual aliasing, viewpoint shift, deformation, and scale transform.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-12-air-series/AirObject.gif&quot; /&gt;
    &lt;figcaption&gt;
        Temporal object matching on videos.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirDOS: Dynamic Object-aware SLAM (DOS) system&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first DOS system showing that camera pose estimation can be improved by incorporating dynamic articulated objects.&lt;/li&gt;
      &lt;li&gt;Establish 4-D dynamic object maps.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-12-air-series/AirDOS.gif&quot; /&gt;
    &lt;figcaption&gt;
        Dynamic Objects can correct the camera pose estimation.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirLoop: Lifelong Learning for Robots&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first lifelong learning method for loop closure detection.&lt;/li&gt;
      &lt;li&gt;Model incremental improvement even after deployment.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-09-28-airloop/tartanair-ll.gif&quot; /&gt;
    &lt;figcaption&gt;
        The model is able to correct previously made mistakes after learning more environments.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirCode: Robust Object Encoding&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first deep point-based object encoding for single image.&lt;/li&gt;
      &lt;li&gt;It achieves the state-of-the-art performance for object re-identification.&lt;/li&gt;
      &lt;li&gt;Robust to viewpoint shift, object deformation, and scale transform.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching1.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching2.gif&quot; /&gt;
    &lt;figcaption&gt;
        A human matching demo.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Congratulations to the above young researchers!&lt;/p&gt;

&lt;p&gt;More information can be found at the &lt;a href=&quot;/research&quot;&gt;research page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some project pages will be released soon.&lt;/p&gt;</content><author><name>Chen Wang</name></author><category term="research" /><summary type="html">Air Series is a collection of articles mentored by Chen Wang.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-03-12-air-series/AirDet-16x9.gif" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-03-12-air-series/AirDet-16x9.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">3D Human Reconstruction with Collaborative Aerial Cameras</title><link href="https://theairlab.org/multidrone/" rel="alternate" type="text/html" title="3D Human Reconstruction with Collaborative Aerial Cameras" /><published>2022-03-07T12:00:00+00:00</published><updated>2022-03-07T12:00:00+00:00</updated><id>https://theairlab.org/multidrone</id><content type="html" xml:base="https://theairlab.org/multidrone/">&lt;p&gt;Aerial vehicles are revolutionizing applications that require capturing the 3D structure of dynamic targets in the wild, such as sports, medicine and entertainment. The core challenges in developing a motion-capture system that operates in outdoors environments are: (1) 3D inference requires multiple simultaneous viewpoints of the target, (2) occlusion caused by obstacles is frequent when tracking moving targets, and (3) the camera and vehicle state estimation is noisy. We present a real-time aerial system for multi-camera control that can reconstruct human motions in natural environments without the use of special-purpose markers.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-07-multidrone/poster-demo.gif&quot; /&gt;
    &lt;figcaption&gt;
        We present a multi-drone motion capture system for 3D human reconstruction in the wild. Our framework coordinates 
aerial cameras to optimally reconstruct the target’s body pose while avoiding obstacles and occlusions outdoors.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;multi-camera-coordination&quot;&gt;Multi-camera coordination&lt;/h2&gt;

&lt;p&gt;We formulate a multi-camera coordination scheme with the goal of maximizing the reconstructed 3D pose quality of dynamic targets. We develop a scalable two-stage system with long planning time horizons and real-time performance that uses a centralized planner for formation control and a decentralized trajectory optimizer that runs on each robot.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-07-multidrone/real-life-flight.png&quot; /&gt;
    &lt;figcaption&gt;
        Real-life flight among obstacle. Our adaptive formation rotates clockwise avoiding the mound to maintain optimal reconstruction angle.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We provide studies evaluating system performance in simulation, and validate real-world performance using two drones while a target performs activities such as jogging and playing soccer.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-07-multidrone/reconstruction-dynamic.png&quot; /&gt;
    &lt;figcaption&gt;
        3D reconstruction of a highly dynamic target
playing soccer.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/jxt91vx0cns&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;additional-info&quot;&gt;Additional Info&lt;/h2&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{ho2021_human3d,
  author = {Ho, Cherie and Jong, Andrew and Freeman, Harry and Rao, Rohan and Bonatti, Rogerio and Scherer, Sebastian},
  title = {3D Human Reconstruction in the Wild with Collaborative Aerial Cameras},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2021},
  month = sep,
  url = {https://arxiv.org/abs/2108.03936},
  video = {https://youtu.be/jxt91vx0cns}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/pdf/2108.03936.pdf&quot;&gt;paper&lt;/a&gt; for details.&lt;/p&gt;

&lt;h3 id=&quot;contributors&quot;&gt;Contributors&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cherieho.com&quot;&gt;Cherie Ho&lt;/a&gt; &amp;lt;cherieh [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;Andrew Jong&lt;/li&gt;
  &lt;li&gt;Harry Freeman&lt;/li&gt;
  &lt;li&gt;Rohan Rao&lt;/li&gt;
  &lt;li&gt;Rogerio Bonatti&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Prof. Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rebecca Martin</name></author><category term="research" /><summary type="html">Aerial vehicles are revolutionizing applications that require capturing the 3D structure of dynamic targets in the wild, such as sports, medicine and entertainment. The core challenges in developing a motion-capture system that operates in outdoors environments are: (1) 3D inference requires multiple simultaneous viewpoints of the target, (2) occlusion caused by obstacles is frequent when tracking moving targets, and (3) the camera and vehicle state estimation is noisy. We present a real-time aerial system for multi-camera control that can reconstruct human motions in natural environments without the use of special-purpose markers. We present a multi-drone motion capture system for 3D human reconstruction in the wild. Our framework coordinates aerial cameras to optimally reconstruct the target’s body pose while avoiding obstacles and occlusions outdoors.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-03-07-multidrone/poster-demo.gif" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-03-07-multidrone/poster-demo.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Lifelong Graph Learning</title><link href="https://theairlab.org/lgl/" rel="alternate" type="text/html" title="Lifelong Graph Learning" /><published>2022-03-05T12:00:00+00:00</published><updated>2022-03-05T12:00:00+00:00</updated><id>https://theairlab.org/lgl</id><content type="html" xml:base="https://theairlab.org/lgl/">&lt;p&gt;Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training. However, graph-structured data is often formed in a streaming fashion so that learning a graph continuously is often necessary.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/growing-graph.jpg&quot; /&gt;
    &lt;figcaption&gt;
        A temporally growing graph, which is challenging to learn the graph in a sequential way.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Existing lifelong learning techniques are mostly designed for convolutional neural networks (CNNs), which &lt;em&gt;assumes the new data samples are independent&lt;/em&gt;. However, in lifelong graph learning, &lt;em&gt;nodes are connected and dynamically added&lt;/em&gt;.
In this work, we have an important observation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The number of nodes increases dynamically, while the number of node features is stable.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, we introduce feature graph, which &lt;strong&gt;takes features as nodes and turns nodes into independent graphs&lt;/strong&gt;. This successfully converts the original problem of node classification to graph classification, in which the increasing nodes are turned into independent training samples.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/feature-graph.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Feature graph takes the features as nodes and turns nodes into graphs, resulting in a graph predictor instead of the node predictor. Take the node a with label &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathbf{z}_a&quot; /&gt; in the regular graph &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{G}&quot; /&gt; as an example, its features &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=x_a = [1, 0, 0, 1]&quot; /&gt;  are nodes &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\{a1, a2, a3, a4\}&quot; /&gt; in feature graph &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{G}^F&quot; /&gt;. 
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The feature adjacency is established via feature cross-correlation between &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=a&quot; /&gt; and its neighbors &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{N}(a) = \{a, b, c, d, e\}&quot; /&gt; to model feature “interaction.”  This makes the lifelong learning techniques for CNN applicable to GNN, as the new nodes in a regular graph become individual training samples.&lt;/p&gt;

&lt;figure&gt;
    &lt;figcaption&gt;
        The relationship of a graph and feature graph.
    &lt;/figcaption&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/relationship.jpg&quot; /&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;applications&quot;&gt;Applications&lt;/h2&gt;

&lt;h4 id=&quot;1-feature-matching&quot;&gt;1. Feature Matching&lt;/h4&gt;

&lt;p&gt;Image feature matching is crucial for many 3-D computer vision tasks including simultaneous localization and mapping (SLAM).
As shown below, the interest point and their descriptors form an infinite temporal growing graph, in which the feature points are nodes and their descriptors are the node features. In this way, the problem of feature matching becomes edge prediction for a temporal growing graph.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/matching.jpg&quot; /&gt;
    &lt;figcaption&gt;
         Feature matching is a problem of edge prediction for temporal growing graph.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In an environment with dynamic illumination, feature graph network can also achieve much better accuracy and robustness, which demonstrate its effectiveness.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/matching.gif&quot; /&gt;
    &lt;figcaption&gt;
        Feature fatching in an environment with dynamic illumination.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;2-distributed-human-action-recognition-with-wearable-devices&quot;&gt;2. Distributed Human Action Recognition with Wearable Devices&lt;/h4&gt;

&lt;p&gt;Five sensors, each of which consists of a triaxial accelerometer and a biaxial gyroscope, are located at the left and right forearms, waist, left and right ankles, respectively. Each sensor produces 5 data streams and totally 5 × 5 data streams is available. 13 daily action categories are considered, including rest at standing (ReSt), rest at sitting (ReSi), rest at lying (ReLi), walk forward (WaFo), walk forward left-circle (WaLe), walk forward right-circle (WaRi), turn left (TuLe), turn right (TuRi), go upstairs (Up), go downstairs (Down), jog (Jog), jump (Jump), and push wheelchair (Push). Therefore, action recognition is a problem of sub-graph classification.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/ward.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Lifelong human action recognition is a problem of lifelong sub-graph classification.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Our feature graph network (FGN) has a much higher and stable performance than all the other
methods, including GCN, APPNP, and GAT. It also achieves a much higher final per-class precision in nearly all the categories.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;FGN is the first method to bridge graph learning to lifelong learning via a novel graph topology.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Chen Wang, Yuheng Qiu, Dasong Gao, Sebastian Scherer. &lt;a href=&quot;https://arxiv.org/pdf/2009.00647.pdf&quot;&gt;Lifelong Graph Learning&lt;/a&gt;. “&lt;em&gt;2022 Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2022.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.00647.pdf&quot;&gt;Download PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;source-code&quot;&gt;Source Code&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/wang-chen/LGL&quot;&gt;Lifelong Graph Learning (Citation Graph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/wang-chen/lgl-action-recognition&quot;&gt;Lifelong Graph Learning (Action Recognition)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/wang-chen/lgl-feature-matching&quot;&gt;Lifelong Graph Learning (Feature Matching)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{wang2022lifelong,
 title={Lifelong graph learning},
 author={Wang, Chen and Gao, Dasong and Qiu, Yuheng and Scherer, Sebastian},
 booktitle={2022 Conference on Computer Vision and Pattern Recognition (CVPR)},
 year={2022}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Chen Wang</name></author><category term="research" /><summary type="html">Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training. However, graph-structured data is often formed in a streaming fashion so that learning a graph continuously is often necessary.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-03-05-lgl/matching.jpg" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-03-05-lgl/matching.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Wire detection dataset</title><link href="https://theairlab.org/wire-detection/" rel="alternate" type="text/html" title="Wire detection dataset" /><published>2022-02-06T18:53:00+00:00</published><updated>2022-02-06T18:53:00+00:00</updated><id>https://theairlab.org/wire-detection-dataset</id><content type="html" xml:base="https://theairlab.org/wire-detection/">&lt;!-- ## Introduction ##  --&gt;
&lt;p&gt;This dataset is part of the work done for the paper “Wire Detection using Synthetic Data and Dilated Convolutional Networks for Unmanned Aerial Vehicles” puslished at IROS 2017. The authors of the paper are &lt;a href=&quot;https://theairlab.org/team/alumni/ratnesh_madaan/&quot;&gt;Ratnesh Madaan&lt;/a&gt;, Daniel Maturana, and Sebastian Scherer.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;The dataset provides pixel labels for wires such as power lines.&lt;/p&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/https://youtu.be/z6sPz-WPCWQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-02-06-wire-detection/paper_screenshot_01.png&quot; style=&quot;width:99%&quot; /&gt;
 &lt;figcaption&gt;
A few samples from our synthetically generated dataset along with ground truth labels of wires.
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;data-structure&quot;&gt;Data Structure&lt;/h3&gt;
&lt;p&gt;The data samples are saved in individual folders. In each folder, we have the follwing files.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0380/
├── ground_truth_viz.png
├── labeled_ground_truth.png
├── labels.ground
└── original_image.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The pixel values saved in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;labeled_ground_truth.png&lt;/code&gt; are defined as&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1: non-wire pixel.&lt;/li&gt;
  &lt;li&gt;2: wire pixel.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The labels are visualized in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ground_truth_viz.png&lt;/code&gt; as a black-and-white image. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;labels.ground&lt;/code&gt; file is a text file showing the pixel coordinates of the end points of the individual lines.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-02-06-wire-detection/labels_groundtruth.png&quot; width=&quot;200pix&quot; /&gt;
 &lt;figcaption&gt;
Content of the labels.ground file.
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- Example images/gif --&gt;

&lt;h3 id=&quot;download&quot;&gt;Download&lt;/h3&gt;
&lt;p&gt;The data can be downloaded directly from &lt;a href=&quot;https://drive.google.com/file/d/1VXak_nKDszabQvDQQ1AG2bvmBlU7p18J/view?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{madaan2017wire,
  title={Wire detection using synthetic data and dilated convolutional networks for unmanned aerial vehicles},
  author={Madaan, Ratnesh and Maturana, Daniel and Scherer, Sebastian},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3487--3494},
  year={2017},
  organization={IEEE}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;related-work-also-from-the-airlab&quot;&gt;Related work also from the AirLab&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{Madaan:2019kb,
  author = {Madaan, Ratnesh and Kaess, Michael and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2019.8793852},
  isbn = {9781538660263},
  issn = {10504729},
  month = may,
  pages = {5657--5664},
  title = {Multi-view reconstruction of wires using a catenary model},
  year = {2019}
}

@inproceedings{Dubey-2018-107515,
  author = {Dubey, Geetesh and Madaan, Ratnesh and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2018.8593499},
  isbn = {9781538680940},
  issn = {21530866},
  month = oct,
  pages = {6311--6318},
  title = {DROAN - Disparity-Space Representation for Obstacle Avoidance: Enabling Wire Mapping Avoidance},
  year = {2018}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://theairlab.org/team/yaoyuh/&quot;&gt;Yaoyu Hu (editor)&lt;/a&gt; - &lt;a href=&quot;mailto:yaoyuh@andrew.cmu.edu&quot;&gt;yaoyuh@andrew.cmu.edu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; - &lt;a href=&quot;mailto:basti@andrew.cmu.edu&quot;&gt;basti@andrew.cmu.edu&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;related-links&quot;&gt;Related links&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://madratman.github.io/wire_detection_iros_2017/&quot;&gt;Retnesh’s blog post&lt;/a&gt;&lt;/p&gt;

&lt;!-- ### Acknowledgments ### --&gt;

&lt;h3 id=&quot;term-of-use&quot;&gt;Term of use&lt;/h3&gt;

&lt;p&gt;&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border-width:0&quot; src=&quot;https://i.creativecommons.org/l/by/4.0/80x15.png&quot; /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;</content><author><name>Edited by Yaoyu Hu</name></author><category term="datasets" /><summary type="html">This dataset is part of the work done for the paper “Wire Detection using Synthetic Data and Dilated Convolutional Networks for Unmanned Aerial Vehicles” puslished at IROS 2017. The authors of the paper are Ratnesh Madaan, Daniel Maturana, and Sebastian Scherer.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-02-06-wire-detection/paper_screenshot_01.png" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-02-06-wire-detection/paper_screenshot_01.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AirDOS: Dynamic SLAM benefits from Articulated Objects</title><link href="https://theairlab.org/airdos/" rel="alternate" type="text/html" title="AirDOS: Dynamic SLAM benefits from Articulated Objects" /><published>2022-02-06T12:00:00+00:00</published><updated>2022-02-06T12:00:00+00:00</updated><id>https://theairlab.org/airdos</id><content type="html" xml:base="https://theairlab.org/airdos/">&lt;p&gt;Dynamic Object-aware SLAM (DOS) exploits
object-level information to enable robust motion estimation in
dynamic environments. It has attracted increasing attention
with the recent success of learning-based models. Existing
methods mainly focus on identifying and excluding dynamic
objects from the optimization. In this paper, we show that
feature-based visual SLAM systems can also benefit from the
presence of dynamic articulated objects by taking advantage of
two observations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The 3D structure of an articulated object
remains consistent over time;&lt;/li&gt;
  &lt;li&gt;The points on the same object
follow the same motion.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-airdos/airdos.gif&quot; /&gt;
    &lt;figcaption&gt;
        AirDOS demo in TartanAir Shibuya dataset
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In particular, we present &lt;strong&gt;AirDOS&lt;/strong&gt;,
a dynamic object-aware system that introduces rigidity and
motion constraints to model articulated objects. By jointly
optimizing the camera pose, object motion, and the object 3D
structure, we can rectify the camera pose estimation, preventing
tracking lost, and generate 4D spatio-temporal maps for both
dynamic objects and static scenes.&lt;/p&gt;

&lt;h2 id=&quot;articulated-objects&quot;&gt;Articulated Objects&lt;/h2&gt;

&lt;p&gt;In this paper, we extend the simple rigid objects to general articulated objects, defined as
objects composed of one or more rigid parts (links) connected by joints allowing rotational or
translational motion, e.g., vehicles and humans, and utilize the properties of
articulated objects to improve the camera pose estimation. Namely, we jointly optimize the
3D structural information and the motion of articulated objects. To this end, we introduce (1)
a &lt;strong&gt;rigidity constraint&lt;/strong&gt;, which assumes that the distance between any two points located on the
same rigid part remains constant over time, and (2) a &lt;strong&gt;motion constraint&lt;/strong&gt;, which assumes that
feature points on the same rigid parts follow the same 3D motion. This allows us to build a 4D
spatio-temporal map including both dynamic and static structures.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-airdos/airticulated.jpg&quot; /&gt;
    &lt;figcaption&gt;
        This is an example of the articulated dynamic objects’ point-segment model. In urban
environment, we can model rigid objects like vehicle and semi-rigid objects like pedestrian as articulated
object.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/gM5iUL1B6R0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/abs/2109.09903&quot;&gt;Paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/haleqiu/AirDOS&quot;&gt;Code&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/yuheng/&quot;&gt;Yuheng Qiu&lt;/a&gt; &amp;lt;yuhengq [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;Wenshan Wang &amp;lt;wenshanw [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yuheng Qiu</name></author><category term="research" /><summary type="html">Dynamic Object-aware SLAM (DOS) exploits object-level information to enable robust motion estimation in dynamic environments. It has attracted increasing attention with the recent success of learning-based models. Existing methods mainly focus on identifying and excluding dynamic objects from the optimization. In this paper, we show that feature-based visual SLAM systems can also benefit from the presence of dynamic articulated objects by taking advantage of two observations: The 3D structure of an articulated object remains consistent over time; The points on the same object follow the same motion.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2022-02-06-airdos/airdos.gif" /><media:content medium="image" url="https://theairlab.org/img/posts/2022-02-06-airdos/airdos.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AirCode: A Robust Object Encoding Method</title><link href="https://theairlab.org/aircode/" rel="alternate" type="text/html" title="AirCode: A Robust Object Encoding Method" /><published>2021-10-06T12:00:00+00:00</published><updated>2021-10-06T12:00:00+00:00</updated><id>https://theairlab.org/aircode</id><content type="html" xml:base="https://theairlab.org/aircode/">&lt;p&gt;Object encoding and identification is crucial for many robotic tasks such as autonomous exploration and semantic relocalization. Existing works heavily rely on the tracking of detected objects but have difficulty to recall revisited objects precisely. In this work, we propose a novel object encoding method, &lt;strong&gt;AirCode&lt;/strong&gt;, based on a graph of key-points. To be robust to the number of key-points detected, we propose a feature sparse encoding and object dense encoding method to ensure that each key-point can only affect a small part of the object descriptors, leading it to be robust to viewpoint changes, scaling, occlusion, and even object deformation.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching1.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching2.gif&quot; /&gt;
    &lt;figcaption&gt;
        A human matching demo.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;object-representation&quot;&gt;Object Representation&lt;/h2&gt;

&lt;p&gt;To save computational resources, object matching in SLAM is often based on key-point features, as the feature-based SLAM methods are still widely used. Inspired by the recent progresses in deep learning-based key-point detector and feature matching methods, it becomes intuitive to encode an object via a group of key-points in an end-to-end manner, where the key-points on the same object form a graph neural network. Therefore, we can take the graph embeddings as the object descriptors.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/demo.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Four objects including 1 rigid object (laptop) and 3 non-rigid objects (human) are identified. AirCode is insensitive to viewpoint (Obj 1 in (b) and (d)), scaling (Obj 3 and Obj 4), occlusion (Obj 4), and even posture change (Obj 2 and Obj 3).
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;During robot exploration, robots often observe part of the objects due to occlusion and different viewpoints, resulting in that the object key-points only have a small overlap across different frames. Therefore, the key-points graph embedding will be easily affected, which makes it difficult to directly apply a graph network. To solve this problem, we argue that a key-point descriptor should have a sparse effect on the object embedding. This means that only a few positions of an object descriptor can be affected if a key-point is added or removed from an object graph. To achieve this, we propose a sparse object encoding method, which is robust to the change of viewpoint and object deformation.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/kitti-relocalization.gif&quot; /&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/ZhW4Qk1tLNQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Kuan Xu, Chen Wang, Chao Chen, Wei Wu, Sebastian Scherer. AirCode: A Robust Object Encoding Method.” &lt;em&gt;IEEE Robotics and Automation Letters (RA-L)&lt;/em&gt;, 2022. (Accepted to ICRA 2022)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/pdf/2105.00327&quot;&gt;Paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/wang-chen/AirCode&quot;&gt;Code&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Chen Wang</name></author><category term="research" /><summary type="html">Object encoding and identification is crucial for many robotic tasks such as autonomous exploration and semantic relocalization. Existing works heavily rely on the tracking of detected objects but have difficulty to recall revisited objects precisely. In this work, we propose a novel object encoding method, AirCode, based on a graph of key-points. To be robust to the number of key-points detected, we propose a feature sparse encoding and object dense encoding method to ensure that each key-point can only affect a small part of the object descriptors, leading it to be robust to viewpoint changes, scaling, occlusion, and even object deformation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://theairlab.org/img/posts/2021-10-06-aircode/object-matching1.gif" /><media:content medium="image" url="https://theairlab.org/img/posts/2021-10-06-aircode/object-matching1.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>